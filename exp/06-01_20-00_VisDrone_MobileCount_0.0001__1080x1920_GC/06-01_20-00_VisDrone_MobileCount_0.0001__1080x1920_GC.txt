from easydict import EasyDict
import time

__C = EasyDict()
cfg = __C

__C.SEED = 3035  # random seed

# System settings
__C.TRAIN_BATCH_SIZE = 2
__C.VAL_BATCH_SIZE = 6
__C.TEST_BATCH_SIZE = 2
__C.N_WORKERS = 2

__C.PRE_TRAINED = None

# path settings
__C.EXP_PATH = '../exp'
__C.DATASET = 'VisDrone'
__C.NET = 'MobileCount'
__C.DETAILS = '_1080x1920_GC'

# learning optimizer settings
__C.LR = 1e-4  # learning rate
__C.W_DECAY = 1e-4  # weight decay
__C.LR_DECAY = 0.995  # decay rate
__C.LR_DECAY_START = 0  # when training epoch is more than it, the learning rate will be begin to decay
__C.NUM_EPOCH_LR_DECAY = 1  # decay frequency
__C.MAX_EPOCH = 500

__C.OPTIM_ADAM = ('Adam',
                  {
                      'lr': __C.LR,
                      'weight_decay': __C.W_DECAY,
                  })
__C.OPTIM_SGD = ('SGD',
                  {
                      'lr': __C.LR,
                      'weight_decay': __C.W_DECAY,
                      'momentum': 0.95
                  })

__C.OPTIM = __C.OPTIM_ADAM  # Chosen optimizer

__C.PATIENCE = 15
__C.EARLY_STOP_DELTA = 1e-2

# print
__C.PRINT_FREQ = 10

now = time.strftime("%m-%d_%H-%M", time.localtime())
__C.EXP_NAME = now \
               + '_' + __C.DATASET \
               + '_' + __C.NET \
               + '_' + str(__C.LR) \
               + '_' + __C.DETAILS
__C.DEVICE = 'cuda'  # cpu or cuda

# ------------------------------VAL------------------------
__C.VAL_SIZE = 0.2
__C.VAL_DENSE_START = 1
__C.VAL_FREQ = 10  # Before __C.VAL_DENSE_START epoches, the freq is set as __C.VAL_FREQ




===============+++++++++++++++===============

all_ep_1_mae_41.6_mse_58.2
    [mae 41.57 mse 58.23], [val loss 3.5239]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_3_mae_58.1_mse_82.4
    [mae 58.10 mse 82.45], [val loss 3.2659]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_4_mae_64.0_mse_95.5
    [mae 63.96 mse 95.45], [val loss 3.2161]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_5_mae_43.7_mse_61.1
    [mae 43.70 mse 61.10], [val loss 2.9241]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_6_mae_24.3_mse_37.5
    [mae 24.30 mse 37.49], [val loss 2.8296]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_7_mae_41.8_mse_58.7
    [mae 41.82 mse 58.70], [val loss 2.8390]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_8_mae_20.9_mse_35.5
    [mae 20.94 mse 35.53], [val loss 2.7666]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_9_mae_24.6_mse_42.6
    [mae 24.60 mse 42.59], [val loss 2.7429]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_10_mae_62.3_mse_82.1
    [mae 62.26 mse 82.10], [val loss 2.8950]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_11_mae_30.8_mse_40.4
    [mae 30.81 mse 40.43], [val loss 2.6739]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_12_mae_20.5_mse_32.0
    [mae 20.46 mse 31.95], [val loss 2.5793]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_13_mae_50.2_mse_69.1
    [mae 50.15 mse 69.12], [val loss 2.6031]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_14_mae_16.3_mse_25.8
    [mae 16.31 mse 25.82], [val loss 2.4857]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_15_mae_30.9_mse_44.9
    [mae 30.94 mse 44.88], [val loss 2.5214]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_16_mae_18.3_mse_26.9
    [mae 18.30 mse 26.88], [val loss 2.4188]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_17_mae_28.0_mse_39.6
    [mae 28.01 mse 39.61], [val loss 2.4508]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_18_mae_27.7_mse_39.3
    [mae 27.70 mse 39.30], [val loss 2.4774]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_19_mae_24.1_mse_35.7
    [mae 24.13 mse 35.71], [val loss 2.4172]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_20_mae_35.7_mse_47.3
    [mae 35.67 mse 47.29], [val loss 2.4076]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_21_mae_33.2_mse_48.1
    [mae 33.17 mse 48.07], [val loss 2.4196]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_22_mae_24.5_mse_37.8
    [mae 24.53 mse 37.80], [val loss 2.3605]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_23_mae_26.7_mse_41.3
    [mae 26.65 mse 41.27], [val loss 2.3017]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_24_mae_19.7_mse_29.5
    [mae 19.68 mse 29.55], [val loss 2.3034]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_25_mae_22.6_mse_35.7
    [mae 22.59 mse 35.75], [val loss 2.3768]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_26_mae_36.6_mse_54.6
    [mae 36.62 mse 54.61], [val loss 2.3734]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_27_mae_39.8_mse_55.9
    [mae 39.78 mse 55.91], [val loss 2.6044]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_28_mae_19.1_mse_29.9
    [mae 19.14 mse 29.85], [val loss 2.2103]
===============+++++++++++++++===============

===============+++++++++++++++===============

all_ep_29_mae_17.7_mse_26.6
    [mae 17.68 mse 26.62], [val loss 2.2503]
===============+++++++++++++++===============

